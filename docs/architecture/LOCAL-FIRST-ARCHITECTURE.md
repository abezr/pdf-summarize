# Local-First Architecture - PDF Summary AI

**Focus**: Local deployment, minimal external dependencies, cost-optimized LLM usage  
**Philosophy**: Run entirely on localhost, only LLM API calls are external  
**Date**: 2025-11-27

---

## ğŸ¯ Architecture Philosophy

### Core Principles

1. **Local-First**: Everything runs on your machine
2. **Zero External Services**: No cloud databases, no cloud storage, no SaaS dependencies
3. **Free-Tier Only**: All local services use free, open-source software
4. **LLM Cost Optimization**: Extreme care with OpenAI/Google API tokens
5. **Self-Contained**: Can run completely offline (except LLM API calls)

---

## ğŸ—ï¸ System Architecture

### High-Level Overview

### Mermaid - Local-First Overview

```mermaid
flowchart LR
  user([User Browser\nlocalhost]) --> api[Express API\nlocalhost:3000]
  api --> pipeline[Processing Pipeline\npdf-parse | Tesseract | graph | embeddings]
  api --> cache[(Cache\nnode-cache)]
  api --> db[(SQLite\n./data/database.sqlite)]
  api --> fs[(Local FS\n./data/uploads\n./data/graphs\n./data/embeddings\n./data/cache)]
  pipeline --> fs
  pipeline --> db
  pipeline --> cache
  pipeline -->|LLM calls| llm[(LLM APIs\nOpenAI / Gemini)]
  fs -. cached inputs/outputs .-> pipeline
  db -. metadata/summaries .-> api
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LOCAL MACHINE ONLY                        â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  Browser â”‚  http://localhost:3000                         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚       â”‚                                                       â”‚
â”‚       v                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    Express API Server (Node.js)          â”‚               â”‚
â”‚  â”‚    Port: 3000                            â”‚               â”‚
â”‚  â”‚    â€¢ PDF Upload Handler                  â”‚               â”‚
â”‚  â”‚    â€¢ Processing Pipeline Controller       â”‚               â”‚
â”‚  â”‚    â€¢ Summary API                         â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚             â”‚                                                 â”‚
â”‚             v                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    Processing Services                   â”‚               â”‚
â”‚  â”‚    â€¢ PDF Parser (pdf-parse)              â”‚               â”‚
â”‚  â”‚    â€¢ OCR (Tesseract.js - local)         â”‚               â”‚
â”‚  â”‚    â€¢ Graph Builder (in-memory)           â”‚               â”‚
â”‚  â”‚    â€¢ Embedding (local model)             â”‚               â”‚
â”‚  â”‚    â€¢ MCP Context Retriever               â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚             â”‚                                                 â”‚
â”‚             v                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    Local Storage (File System)           â”‚               â”‚
â”‚  â”‚    â€¢ PDFs: ./data/uploads/               â”‚               â”‚
â”‚  â”‚    â€¢ Graphs: ./data/graphs/              â”‚               â”‚
â”‚  â”‚    â€¢ Embeddings: ./data/embeddings/      â”‚               â”‚
â”‚  â”‚    â€¢ Metadata: ./data/metadata.json      â”‚               â”‚
â”‚  â”‚    â€¢ Cache: ./data/cache/                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    Optional: SQLite (Local DB)           â”‚               â”‚
â”‚  â”‚    â€¢ Document metadata                    â”‚               â”‚
â”‚  â”‚    â€¢ Processing history                   â”‚               â”‚
â”‚  â”‚    â€¢ User sessions                        â”‚               â”‚
â”‚  â”‚    File: ./data/database.sqlite          â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ ONLY EXTERNAL CALL
                          v
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LLM API (External)    â”‚
              â”‚   â€¢ OpenAI (GPT-4o)     â”‚
              â”‚   â€¢ Google (Gemini)     â”‚
              â”‚   Use with EXTREME care â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Technology Stack (All Local/Free)

### Core Services

| Component | Technology | Why | Cost |
|-----------|-----------|-----|------|
| **Runtime** | Node.js 20+ | Fast, widely supported | FREE |
| **Backend** | Express.js | Lightweight, simple | FREE |
| **Frontend** | React 18 + Vite | Modern, fast dev | FREE |
| **Database** | SQLite | File-based, zero-config | FREE |
| **File Storage** | Local filesystem | No S3/GCS needed | FREE |
| **Cache** | Node-cache (in-memory) | Simple, no Redis needed | FREE |
| **PDF Parser** | pdf-parse | Local PDF processing | FREE |
| **OCR** | Tesseract.js | Local OCR (browser/node) | FREE |
| **Embeddings** | transformers.js | Local embeddings (ONNX) | FREE |
| **Graph Storage** | In-memory (JSON) | Fast, simple | FREE |

### LLM Integration (ONLY External Dependency)

| Provider | Usage | Cost Strategy |
|----------|-------|---------------|
| **OpenAI GPT-4o** | Summarization only | Minimize tokens, cache aggressively |
| **Google Gemini 1.5** | Fallback, cost-optimized | Prefer for bulk operations |

---

## ğŸ—„ï¸ Data Storage Strategy

### Local File System Structure

```
/home/user/pdf-summary-ai/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/              # Uploaded PDFs
â”‚   â”‚   â”œâ”€â”€ <doc-id>.pdf
â”‚   â”‚   â””â”€â”€ <doc-id>.pdf
â”‚   â”œâ”€â”€ graphs/               # Serialized knowledge graphs
â”‚   â”‚   â”œâ”€â”€ <doc-id>.graph.json
â”‚   â”‚   â””â”€â”€ <doc-id>.graph.json
â”‚   â”œâ”€â”€ embeddings/           # Cached embeddings
â”‚   â”‚   â”œâ”€â”€ <doc-id>.embeddings.json
â”‚   â”‚   â””â”€â”€ <doc-id>.embeddings.json
â”‚   â”œâ”€â”€ cache/                # LLM response cache
â”‚   â”‚   â”œâ”€â”€ summaries/
â”‚   â”‚   â””â”€â”€ ocr-results/
â”‚   â”œâ”€â”€ metadata.json         # Document metadata index
â”‚   â””â”€â”€ database.sqlite       # Optional: Structured metadata
â”œâ”€â”€ src/                      # Application code
â”œâ”€â”€ .env                      # Configuration (API keys)
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

### SQLite Schema (Optional)

**If using SQLite for metadata:**

```sql
-- Documents table
CREATE TABLE documents (
  id TEXT PRIMARY KEY,
  filename TEXT NOT NULL,
  size INTEGER NOT NULL,
  upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  status TEXT NOT NULL, -- 'processing', 'completed', 'failed'
  pages INTEGER,
  has_text_layer BOOLEAN,
  is_scanned BOOLEAN
);

-- Processing history
CREATE TABLE processing_history (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  document_id TEXT NOT NULL,
  stage TEXT NOT NULL, -- 'parse', 'graph', 'embed', 'summarize'
  status TEXT NOT NULL,
  tokens_used INTEGER DEFAULT 0,
  cost REAL DEFAULT 0.0,
  duration_ms INTEGER,
  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (document_id) REFERENCES documents(id)
);

-- Summaries
CREATE TABLE summaries (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  document_id TEXT NOT NULL,
  content TEXT NOT NULL,
  provider TEXT NOT NULL, -- 'openai', 'google'
  model TEXT NOT NULL,
  tokens_used INTEGER,
  cost REAL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (document_id) REFERENCES documents(id)
);

-- LLM usage tracking (IMPORTANT: Monitor costs)
CREATE TABLE llm_usage (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  date DATE NOT NULL,
  provider TEXT NOT NULL,
  model TEXT NOT NULL,
  operation TEXT NOT NULL, -- 'summarize', 'ocr', 'embed'
  tokens_prompt INTEGER,
  tokens_completion INTEGER,
  tokens_total INTEGER,
  cost REAL,
  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Daily cost summary
CREATE VIEW daily_costs AS
SELECT 
  date,
  provider,
  SUM(tokens_total) as total_tokens,
  SUM(cost) as total_cost,
  COUNT(*) as request_count
FROM llm_usage
GROUP BY date, provider
ORDER BY date DESC;
```

---

## ğŸš« Removed External Dependencies

### What We DON'T Need Anymore

| Service | Original Purpose | Local Replacement |
|---------|------------------|-------------------|
| âŒ **PostgreSQL** | Document metadata | âœ… SQLite (file-based) |
| âŒ **Redis** | Caching, graph storage | âœ… node-cache (in-memory) |
| âŒ **S3/GCS** | PDF storage | âœ… Local filesystem |
| âŒ **Prometheus** | Metrics | âœ… Simple JSON logging |
| âŒ **Grafana** | Dashboards | âœ… CLI reports / Web UI |
| âŒ **Docker Compose** | Multi-container orchestration | âœ… Single Node.js process |
| âŒ **Cloud Embeddings** | Vector embeddings | âœ… transformers.js (local) |
| âŒ **Cloud OCR** | Google Vision API | âœ… Tesseract.js (local) |

---

## ğŸ’° LLM Token Optimization Strategies

### Extreme Cost Optimization

#### 1. Aggressive Caching

```typescript
// Cache LLM responses by content hash
const cache = new Map<string, CachedResponse>();

function getCacheKey(content: string, provider: string): string {
  return createHash('sha256')
    .update(`${provider}:${content}`)
    .digest('hex');
}

async function getCachedOrGenerate(
  content: string,
  provider: string
): Promise<string> {
  const key = getCacheKey(content, provider);
  
  // Check in-memory cache
  if (cache.has(key)) {
    logger.info('Cache HIT - Saved LLM API call!');
    return cache.get(key)!.response;
  }
  
  // Check file-based cache
  const cacheFile = `./data/cache/summaries/${key}.json`;
  if (fs.existsSync(cacheFile)) {
    const cached = JSON.parse(fs.readFileSync(cacheFile, 'utf-8'));
    cache.set(key, cached);
    logger.info('File cache HIT - Saved LLM API call!');
    return cached.response;
  }
  
  // Generate and cache
  const response = await generateSummary(content, provider);
  const cached = { response, timestamp: Date.now() };
  
  cache.set(key, cached);
  fs.writeFileSync(cacheFile, JSON.stringify(cached));
  
  return response;
}
```

#### 2. Smart Content Reduction

```typescript
// Only send essential content to LLM
function prepareMinimalContext(graph: DocumentGraph): string {
  // Extract only key sections, skip redundant content
  const keySections = graph.nodes
    .filter(n => n.type === 'section' && n.importance > 0.7)
    .map(n => n.content)
    .join('\n\n');
  
  // Compress tables (summaries, not full data)
  const tableSummaries = graph.nodes
    .filter(n => n.type === 'table')
    .map(n => `Table: ${n.caption} (${n.rows}x${n.cols})`)
    .join('\n');
  
  return `${keySections}\n\nTables:\n${tableSummaries}`;
}
```

#### 3. Use Cheaper Models Strategically

```typescript
// Use Gemini 1.5 Flash (55x cheaper) for most tasks
const MODEL_STRATEGY = {
  // Critical: High quality needed
  critical: {
    provider: 'openai',
    model: 'gpt-4o',
  },
  // Normal: Balance quality and cost
  normal: {
    provider: 'google',
    model: 'gemini-1.5-pro',
  },
  // Bulk: Cost-optimized
  bulk: {
    provider: 'google',
    model: 'gemini-1.5-flash', // 55x cheaper!
  },
};

function selectModel(documentType: string) {
  if (documentType === 'legal' || documentType === 'financial') {
    return MODEL_STRATEGY.critical;
  } else if (documentType === 'research') {
    return MODEL_STRATEGY.normal;
  } else {
    return MODEL_STRATEGY.bulk;
  }
}
```

#### 4. Local Preprocessing

```typescript
// Do as much as possible locally BEFORE calling LLM
function preprocessDocument(text: string): string {
  // Remove boilerplate (headers, footers, page numbers)
  text = removeBoilerplate(text);
  
  // Extract key sentences (local extractive summarization)
  const keySentences = extractKeySentences(text, 0.3); // Top 30%
  
  // Remove duplicate paragraphs
  text = removeDuplicates(keySentences);
  
  // Compress whitespace
  text = text.replace(/\s+/g, ' ').trim();
  
  return text;
}

function extractKeySentences(text: string, ratio: number): string[] {
  // Simple TF-IDF based extraction (local, no LLM)
  const sentences = text.split(/[.!?]+/);
  const scores = sentences.map(s => calculateImportance(s));
  
  const threshold = scores.sort((a, b) => b - a)[
    Math.floor(scores.length * ratio)
  ];
  
  return sentences.filter((_, i) => scores[i] >= threshold);
}
```

#### 5. Token Usage Monitoring

```typescript
// Track EVERY LLM call
class TokenTracker {
  private usage: LLMUsage[] = [];
  
  async trackCall(
    provider: string,
    model: string,
    operation: string,
    tokensUsed: number,
    cost: number
  ) {
    const record = {
      date: new Date().toISOString().split('T')[0],
      provider,
      model,
      operation,
      tokensUsed,
      cost,
      timestamp: Date.now(),
    };
    
    this.usage.push(record);
    
    // Log to SQLite
    await db.run(
      `INSERT INTO llm_usage (date, provider, model, operation, 
       tokens_prompt, tokens_completion, tokens_total, cost)
       VALUES (?, ?, ?, ?, ?, ?, ?, ?)`,
      [record.date, provider, model, operation, 0, tokensUsed, tokensUsed, cost]
    );
    
    // Alert if daily budget exceeded
    const dailyCost = this.getDailyCost();
    if (dailyCost > 1.0) { // $1/day budget
      logger.warn(`âš ï¸  Daily LLM budget exceeded: $${dailyCost.toFixed(2)}`);
    }
  }
  
  getDailyCost(): number {
    const today = new Date().toISOString().split('T')[0];
    return this.usage
      .filter(u => u.date === today)
      .reduce((sum, u) => sum + u.cost, 0);
  }
  
  getReport(): string {
    const totalCost = this.usage.reduce((sum, u) => sum + u.cost, 0);
    const totalTokens = this.usage.reduce((sum, u) => sum + u.tokensUsed, 0);
    
    return `
LLM Usage Report
================
Total Cost: $${totalCost.toFixed(4)}
Total Tokens: ${totalTokens.toLocaleString()}
Total Calls: ${this.usage.length}

By Provider:
${this.getProviderBreakdown()}

Daily Costs:
${this.getDailyCosts()}
    `.trim();
  }
}
```

---

## ğŸ”§ Local Embeddings (No OpenAI Embeddings API)

### Using Transformers.js (Local ONNX Models)

```typescript
import { pipeline } from '@xenova/transformers';

class LocalEmbeddings {
  private model: any;
  
  async initialize() {
    // Load small embedding model (runs in Node.js)
    this.model = await pipeline(
      'feature-extraction',
      'Xenova/all-MiniLM-L6-v2' // 80MB model, runs locally
    );
  }
  
  async embed(text: string): Promise<number[]> {
    const result = await this.model(text, {
      pooling: 'mean',
      normalize: true,
    });
    
    return Array.from(result.data);
  }
  
  async embedBatch(texts: string[]): Promise<number[][]> {
    return Promise.all(texts.map(t => this.embed(t)));
  }
}

// Usage
const embeddings = new LocalEmbeddings();
await embeddings.initialize();

const vector = await embeddings.embed('This is a test sentence.');
// Returns 384-dim vector, computed locally, $0 cost!
```

**Benefits**:
- âœ… **$0 cost** (no OpenAI embeddings API)
- âœ… **Fast** (runs locally on CPU/GPU)
- âœ… **Private** (data never leaves your machine)
- âœ… **Offline** (works without internet)

---

## ğŸ“Š Observability (Local-First)

### Simple JSON Logging

```typescript
// No Prometheus, just structured JSON logs
const logStream = fs.createWriteStream('./data/logs/app.log', { flags: 'a' });

function logMetric(metric: string, value: number, tags: Record<string, string>) {
  const log = {
    timestamp: new Date().toISOString(),
    metric,
    value,
    tags,
  };
  
  logStream.write(JSON.stringify(log) + '\n');
}

// Log LLM usage
logMetric('llm.tokens.used', tokensUsed, {
  provider: 'openai',
  model: 'gpt-4o',
  operation: 'summarize',
});

// Log processing time
logMetric('processing.duration', durationMs, {
  stage: 'graph-build',
  document_id: docId,
});
```

### CLI Monitoring Dashboard

```typescript
// Simple CLI dashboard (no Grafana needed)
function displayDashboard() {
  const usage = tokenTracker.getReport();
  const docs = getDocumentStats();
  
  console.clear();
  console.log('='.repeat(60));
  console.log('PDF SUMMARY AI - DASHBOARD');
  console.log('='.repeat(60));
  console.log();
  console.log('ğŸ“Š DOCUMENT STATS');
  console.log(`   Total Documents: ${docs.total}`);
  console.log(`   Processed: ${docs.completed}`);
  console.log(`   In Progress: ${docs.processing}`);
  console.log(`   Failed: ${docs.failed}`);
  console.log();
  console.log('ğŸ’° LLM USAGE (TODAY)');
  console.log(`   Total Cost: $${usage.dailyCost.toFixed(4)}`);
  console.log(`   Total Tokens: ${usage.dailyTokens.toLocaleString()}`);
  console.log(`   API Calls: ${usage.dailyCalls}`);
  console.log();
  console.log('âš¡ CACHE HIT RATE');
  console.log(`   Hit Rate: ${usage.cacheHitRate.toFixed(1)}%`);
  console.log(`   Saved Calls: ${usage.savedCalls}`);
  console.log(`   Saved Cost: $${usage.savedCost.toFixed(4)}`);
  console.log();
  console.log('='.repeat(60));
}

// Refresh every 5 seconds
setInterval(displayDashboard, 5000);
```

---

## ğŸš€ Startup Script

### Single Command Launch

```bash
#!/bin/bash
# start.sh - Launch PDF Summary AI (local-first)

echo "ğŸš€ Starting PDF Summary AI (Local-First Mode)"
echo ""

# Check Node.js version
if ! command -v node &> /dev/null; then
  echo "âŒ Node.js not found. Please install Node.js 20+"
  exit 1
fi

# Check API keys
if [ -z "$OPENAI_API_KEY" ] && [ -z "$GOOGLE_API_KEY" ]; then
  echo "âš ï¸  Warning: No LLM API keys found!"
  echo "   Set OPENAI_API_KEY or GOOGLE_API_KEY in .env"
  echo ""
fi

# Create data directories
mkdir -p data/{uploads,graphs,embeddings,cache/summaries,cache/ocr-results,logs}

# Initialize SQLite database
if [ ! -f "data/database.sqlite" ]; then
  echo "ğŸ“¦ Initializing SQLite database..."
  node scripts/init-db.js
fi

# Install dependencies (if needed)
if [ ! -d "node_modules" ]; then
  echo "ğŸ“¦ Installing dependencies..."
  npm install
fi

# Start server
echo "âœ… Starting server on http://localhost:3000"
echo "ğŸ’¡ Press Ctrl+C to stop"
echo ""

npm start
```

---

## ğŸ“ Configuration (.env)

```env
# =============================================================================
# PDF SUMMARY AI - LOCAL-FIRST CONFIGURATION
# =============================================================================

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------
NODE_ENV=development
PORT=3000
HOST=localhost

# -----------------------------------------------------------------------------
# LLM Configuration (ONLY EXTERNAL DEPENDENCY)
# -----------------------------------------------------------------------------
# OpenAI (optional)
OPENAI_API_KEY=sk-your-key
OPENAI_MODEL=gpt-4o

# Google AI (optional)
GOOGLE_API_KEY=your-key
GOOGLE_MODEL=gemini-1.5-pro

# Provider selection
LLM_PROVIDER=auto  # auto, openai, google
LLM_ENABLE_FALLBACK=true

# Cost optimization
LLM_CACHE_ENABLED=true
LLM_MAX_TOKENS=2000  # Reduce to save costs
LLM_TEMPERATURE=0.3
DAILY_BUDGET_USD=1.00  # Alert if exceeded

# -----------------------------------------------------------------------------
# Local Storage Configuration
# -----------------------------------------------------------------------------
DATA_DIR=./data
UPLOAD_DIR=./data/uploads
GRAPH_DIR=./data/graphs
EMBEDDING_DIR=./data/embeddings
CACHE_DIR=./data/cache
LOG_DIR=./data/logs

# Database
DB_TYPE=sqlite  # sqlite or json
DB_PATH=./data/database.sqlite

# File limits
MAX_FILE_SIZE=52428800  # 50MB
MAX_FILES=1000  # Limit to prevent disk issues

# -----------------------------------------------------------------------------
# Local Embeddings Configuration
# -----------------------------------------------------------------------------
EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2  # Local ONNX model
EMBEDDING_CACHE_ENABLED=true

# -----------------------------------------------------------------------------
# OCR Configuration (Local-First)
# -----------------------------------------------------------------------------
OCR_ENABLED=true
OCR_PROVIDER=tesseract  # tesseract (local, free)
OCR_LANGUAGE=eng+spa+fra+deu
OCR_MIN_CONFIDENCE=60

# Vision API (use ONLY for critical cases)
OCR_VISION_ENABLED=false  # Disabled by default to save costs
OCR_VISION_THRESHOLD=90  # Only use if Tesseract < 90% confidence

# -----------------------------------------------------------------------------
# Cache Configuration
# -----------------------------------------------------------------------------
CACHE_TTL=86400  # 24 hours
CACHE_MAX_SIZE=1000  # Max items in memory

# -----------------------------------------------------------------------------
# Monitoring Configuration
# -----------------------------------------------------------------------------
METRICS_ENABLED=true
METRICS_LOG_FILE=./data/logs/metrics.log
ENABLE_DASHBOARD=true  # CLI dashboard

# -----------------------------------------------------------------------------
# Feature Flags
# -----------------------------------------------------------------------------
ENABLE_GRAPH_CACHE=true
ENABLE_EMBEDDING_CACHE=true
ENABLE_SUMMARY_CACHE=true
ENABLE_OCR_CACHE=true
```

---

## ğŸ’¾ Package.json Dependencies (Minimal)

```json
{
  "name": "pdf-summary-ai-local",
  "version": "1.0.0",
  "description": "Local-first PDF summarization with Knowledge Graph",
  "scripts": {
    "start": "node src/server.js",
    "dev": "nodemon src/server.js",
    "init-db": "node scripts/init-db.js",
    "dashboard": "node scripts/dashboard.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "multer": "^1.4.5-lts.1",
    "pdf-parse": "^1.1.1",
    "tesseract.js": "^5.0.4",
    "@xenova/transformers": "^2.10.0",
    "openai": "^4.24.1",
    "@google/generative-ai": "^0.1.3",
    "better-sqlite3": "^9.2.2",
    "node-cache": "^5.1.2",
    "dotenv": "^16.3.1",
    "winston": "^3.11.0"
  },
  "devDependencies": {
    "nodemon": "^3.0.2"
  }
}
```

**Total Size**: ~150MB (including models)

---

## âœ… Benefits of Local-First Architecture

| Benefit | Description |
|---------|-------------|
| âœ… **Zero Cloud Costs** | No AWS, GCP, Azure bills (except LLM API) |
| âœ… **Privacy** | Data never leaves your machine |
| âœ… **Fast Setup** | `npm install && npm start` |
| âœ… **Offline Capable** | Works without internet (except LLM calls) |
| âœ… **Predictable Costs** | Only LLM API usage, strictly monitored |
| âœ… **Simple Deployment** | Single Node.js process |
| âœ… **Easy Debugging** | All data on filesystem |
| âœ… **No Vendor Lock-in** | 100% portable |
| âœ… **Low Maintenance** | No cloud infrastructure to manage |

---

## ğŸ¯ Cost Breakdown

### Monthly Cost Estimate (1,000 Documents)

| Service | Original Cost | Local-First Cost | Savings |
|---------|---------------|------------------|---------|
| **PostgreSQL (Cloud)** | $15-25/month | $0 (SQLite) | $15-25 |
| **Redis (Cloud)** | $10-15/month | $0 (node-cache) | $10-15 |
| **S3/GCS Storage** | $5-10/month | $0 (local disk) | $5-10 |
| **Prometheus/Grafana** | $20-50/month | $0 (JSON logs) | $20-50 |
| **Cloud Embeddings** | $10-20/month | $0 (local model) | $10-20 |
| **Cloud OCR** | $15-150/month | $0 (Tesseract) | $15-150 |
| **LLM API (optimized)** | $5-20/month | $5-20/month | $0 |
| **TOTAL** | **$80-290/month** | **$5-20/month** | **$60-270** |

**Savings**: **75-95% reduction in costs!** ğŸ‰

---

## ğŸ”„ Migration Path

### From Cloud Architecture to Local-First

1. **Replace PostgreSQL with SQLite**
   - Export data to JSON
   - Import to SQLite
   - Update queries (minimal changes)

2. **Replace Redis with node-cache**
   - Remove Redis connection code
   - Use in-memory cache
   - Optional: Add file-based persistence

3. **Replace S3/GCS with Local Filesystem**
   - Copy files from cloud to `./data/uploads/`
   - Update file paths
   - Add disk space monitoring

4. **Replace Prometheus with JSON Logging**
   - Remove Prometheus client
   - Use simple JSON logs
   - Build CLI dashboard

5. **Replace OpenAI Embeddings with Local Model**
   - Install transformers.js
   - Download ONNX model (one-time)
   - Update embedding code

---

## ğŸ“š Next Steps

1. **Implement Local Storage Layer**
   - File-based document store
   - SQLite integration
   - Cache management

2. **Add Token Optimization**
   - Aggressive caching
   - Content reduction
   - Smart model selection

3. **Local Embeddings Integration**
   - transformers.js setup
   - ONNX model download
   - Vector similarity search

4. **Build CLI Dashboard**
   - Real-time metrics
   - Cost tracking
   - Cache hit rates

5. **Documentation Updates**
   - Local setup guide
   - Cost optimization tips
   - Troubleshooting

---

## ğŸš€ Ready for Local Deployment!

This architecture enables:
- âœ… **$0 infrastructure costs** (only LLM API)
- âœ… **Complete privacy** (all data local)
- âœ… **Simple setup** (single command)
- âœ… **Extreme cost optimization** (aggressive caching + local processing)
- âœ… **Production-ready** (with monitoring and error handling)

**Perfect for**: Individual users, small teams, cost-sensitive projects, privacy-focused deployments

---

**Repository**: https://github.com/abezr/pdf-summarize  
**Status**: Local-First Architecture Specified âœ…
